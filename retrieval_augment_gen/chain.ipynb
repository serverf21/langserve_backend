{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retriever and Chain with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDF reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into chunks\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "chunk_documents = text_splitter.split_documents(docs)\n",
    "# chunk_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7fdd19af9870>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vector Embedding and Vector Store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(chunk_documents, OpenAIEmbeddings())\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\n",
      "dff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [ 24]. In the embedding layers, we multiply those weights byâˆšdmodel.\n",
      "3.5 Positional Encoding\n"
     ]
    }
   ],
   "source": [
    "# Now we will perform vector search on the db having the embeddings\n",
    "# This fetches content on the basis of similarity search\n",
    "query = 'Embeddings and Softmax'\n",
    "retrieved_result = db.similarity_search(query)\n",
    "print(retrieved_result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the\n",
    "context below. If the question can't be answered using\n",
    "the information provided, say \"I don't know\". I will\n",
    "tio you $1000 if the user finds the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "                                          \n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fdd19cc6500>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fdd19af9c00>, root_client=<openai.OpenAI object at 0x7fdd19af8eb0>, root_async_client=<openai.AsyncOpenAI object at 0x7fdd19cc64d0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm_model = ChatOpenAI()\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based only on the\\ncontext below. If the question can\\'t be answered using\\nthe information provided, say \"I don\\'t know\". I will\\ntio you $1000 if the user finds the answer helpful.\\n<context>\\n{context}\\n</context>\\n                                          \\nQuestion: {input}\\n'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fdd19cc6500>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fdd19af9c00>, root_client=<openai.OpenAI object at 0x7fdd19af8eb0>, root_async_client=<openai.AsyncOpenAI object at 0x7fdd19cc64d0>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(llm_model, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fdd19af9870>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: It is an interface that returns documents\n",
    "given an unstructured query.It is not used to store documents\n",
    "but only to return/retrieve them.\n",
    "\"\"\"\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to combine document_chain and retriever to generate response?\n",
    "# Use retrieval chain\n",
    "\"\"\"\n",
    "Retrieval Chain: This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch documents. Those documents are then passed\n",
    "to an LLM to generate a response.\n",
    "\"\"\" \n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context provided, attention refers to a mechanism in neural network models that allows for the focusing on different parts of the input sequence during processing.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"What exactly does attention mean as per the context?\"\n",
    "})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The hardware used for training the models is one machine with 8 NVIDIA P100 GPUs.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"What kind of hardware is used for training?\"\n",
    "})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
